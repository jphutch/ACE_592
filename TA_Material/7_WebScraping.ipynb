{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wrong-heath",
   "metadata": {},
   "source": [
    "# Zillow Webscraping + Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-electron",
   "metadata": {},
   "source": [
    "## 1. Webscraping:\n",
    "\n",
    "In this part of the code, I'll show you how to extract a data frame with the houses for sale in Champaign from www.zillow.com. This code only extract the first page of the static webpage. Therefore, the resulting data frame will contain only 40 rows (houses/apts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 40)\n",
    "#pd.set_option('display.max_columns', 500)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "\n",
    "with requests.Session() as s:\n",
    "    city = 'champaign/' \n",
    "    url = 'https://www.zillow.com/homes/for_sale/'+city    \n",
    "    r = s.get(url, headers=req_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-interim",
   "metadata": {},
   "source": [
    "## Create DataFrame based on classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df['price'] = soup.find_all(class_='list-card-price')\n",
    "df['address'] = soup.find_all(class_='list-card-addr')\n",
    "df['beds'] = soup.find_all(\"ul\", class_=\"list-card-details\")\n",
    "\n",
    "#df['link']  = list(soup.find_all(class_= 'list-card-link'))\n",
    "df;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRICES\n",
    "df['price']=[x.get_text() for x in df['price']]\n",
    "df['price'] = df['price'].str.replace(r'\\D', '')  # \\D is regex for non-digit. \n",
    "\n",
    "# ADDRESSES:\n",
    "df['address']=[x.get_text() for x in df['address']]\n",
    "\n",
    "# BEDS - BATHS - SQFEET - TYPE \n",
    "\n",
    "df['beds']=[x.get_text() for x in df['beds']]\n",
    "\n",
    "df[['beds','baths']] = df.beds.str.split(\" bds\",expand=True)\n",
    "df[['baths','sq_feet']] = df.baths.str.split(\" ba\",expand=True)\n",
    "df[['sq_feet','type']] = df.sq_feet.str.split(\" sqft- \",expand=True)\n",
    "\n",
    "\n",
    "# There are alternative ways to get rid of html tags. Here is the manual way: \n",
    "# df['beds'] = df['beds'].astype('str')\n",
    "# df['beds'] = df['beds'].replace('<ul class=\"list-card-details\"><li class=\"\">','', regex=True)\n",
    "# df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->','',regex=True)\n",
    "# df['beds'] = df['beds'].replace('</abbr></li><li class=\"\">','-',regex=True)\n",
    "# df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->','',regex=True)\n",
    "# df['beds'] = df['beds'].replace('</abbr></li><li class=\"\">','-',regex=True)\n",
    "# df['beds'] = df['beds'].replace('</abbr></li><li class=\"list-card-statusText\">','',regex=True)\n",
    "# df['beds'] = df['beds'].replace('</li></ul>','',regex=True)\n",
    "# df[['beds','baths','sq_feet','type','none1']] = df.beds.str.split(\"-\",expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['price','address','beds','baths','sq_feet','type']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'] = df['price'].astype('int')\n",
    "df['beds'] = df['beds'].astype('float')\n",
    "df['baths'] = df['baths'].astype('float')\n",
    "df['sq_feet'] = df['sq_feet'].str.replace(r'\\D', '').astype('float')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-swing",
   "metadata": {},
   "source": [
    "## Obtaining the link of the house/apt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = soup.find_all(class_= 'list-card-link')\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    url = href.get('href')\n",
    "    urls.append(url)\n",
    "\n",
    "df['urls'] = urls\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-neutral",
   "metadata": {},
   "source": [
    "## Webscraping each house/apt link.\n",
    "\n",
    "From each specific link, we will get the descriptions and latitude/longitude of each house/apt for sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use same req_header as before to avoid captchas from Zillow...\n",
    "req_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-ceramic",
   "metadata": {},
   "source": [
    "#### I did a lot of trial and error here...\n",
    "\n",
    "But in summary, I tried with the first link. And then I look inside the results to see where I can find what we want..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['urls'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "with requests.Session() as s:\n",
    "    url = 'https://www.zillow.com/homedetails/1508-S-Mattis-Ave-Champaign-IL-61821/3227757_zpid/'\n",
    "    r2 = s.get(url, headers=req_headers)\n",
    "r2.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(r2.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-thompson",
   "metadata": {},
   "source": [
    "#### Getting lat/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon = soup2.find('script', {'type':'application/ld+json'})\n",
    "latlon = json.loads(latlon.contents[0])\n",
    "latlon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon['geo']['longitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-student",
   "metadata": {},
   "source": [
    "#### Getting Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "#description=soup2.find_all(class_='Text-c11n-8-18-0__aiai24-0 sc-qPwPv cZodDt')\n",
    "description=soup2.find_all(class_='Text-c11n-8-18-0__aiai24-0 sc-qPwPv ielpMy')\n",
    "description = [d.text for d in description]\n",
    "description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-gilbert",
   "metadata": {},
   "source": [
    "## Using a loop to evaluate all the links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "descrip = []\n",
    "descrip2 = []\n",
    "lat = []\n",
    "lon = []\n",
    "\n",
    "for link in df['urls']:\n",
    "    r = s.get(link, headers=req_headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    # Gettting description\n",
    "    description= soup.find_all(class_='Text-c11n-8-18-0__aiai24-0 sc-qPwPv cZodDt')\n",
    "    description = [d.text for d in description]\n",
    "    descrip.append(description)\n",
    "    \n",
    "    # Gettting description - version 2 \n",
    "    description2= soup.find_all(class_='Text-c11n-8-18-0__aiai24-0 sc-qPwPv ielpMy')\n",
    "    description2 = [d.text for d in description2]\n",
    "    descrip2.append(description2)\n",
    "    \n",
    "    \n",
    "    # Getting latitude and longitude:     \n",
    "    latlon = soup.find('script', {'type':'application/ld+json'})\n",
    "    latlon = json.loads(latlon.contents[0])\n",
    "    latitude = latlon['geo']['latitude']\n",
    "    longitude = latlon['geo']['longitude']\n",
    "    \n",
    "    lat.append(latitude)\n",
    "    lon.append(longitude)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "descrip;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-google",
   "metadata": {},
   "source": [
    "### Addind the new columns to our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lat'] = lat\n",
    "df['lon'] = lon\n",
    "# description 1\n",
    "df['descrip'] = descrip\n",
    "df['descrip'] = df['descrip'].astype('str')\n",
    "df['descrip']  = df['descrip'].replace('\\[', '', regex=True)\n",
    "df['descrip']  = df['descrip'].replace('\\]', '', regex=True)\n",
    "\n",
    "# description 2\n",
    "df['descrip2'] = descrip2\n",
    "df['descrip2'] = df['descrip2'].astype('str')\n",
    "df['descrip2']  = df['descrip2'].replace('\\[', '', regex=True)\n",
    "df['descrip2']  = df['descrip2'].replace('\\]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "usedescrip=[len(x)>0 for x in df['descrip']]\n",
    "usedescrip2 = [len(x)>0 for x in df['descrip2']]\n",
    "df.loc[usedescrip,'description'] = df.loc[usedescrip,'descrip']\n",
    "df.loc[usedescrip2,'description'] = df.loc[usedescrip2,'descrip2']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['price', 'address', 'beds', 'baths', 'sq_feet', 'type', 'urls', 'lat','lon', 'description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to save the resulting dataframe: \n",
    "#df.to_csv(r'ZillowWebscrap_Champaign_page1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-alberta",
   "metadata": {},
   "source": [
    "## 2. Text Analysis\n",
    "\n",
    "We'll do text analysis over the description of each listing. However, we will use a file that contains all the 6 pages from Zillow (Instead of only one as we doid above). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-skirt",
   "metadata": {},
   "source": [
    "### Remove punctuation and split words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ZillowWebscrap_Champaign.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lower case variable, and remove \\n (new lines) character in case we have: \n",
    "df['desctiption_lower'] =  df['description'].str.lower().str.replace(\"\\n\",\"\")\n",
    "\n",
    "# Remove punctuation and list of characters that we need to remove \n",
    "remv_punc = str.maketrans('','',string.punctuation + '“' +\"‘\"+'”')\n",
    "\n",
    "df['description_clean'] =  df['desctiption_lower'].str.translate(remv_punc)\n",
    "# Use of regular expressoin to remove digits: \n",
    "df['description_clean'] = [re.sub(\"\\d+\", \"\", x) for x in df['description_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [x.split(\" \") for x in df['description_clean']]\n",
    "words;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-obligation",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to download the stopwords first. I don't need to do that again.\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_list = stopwords.words('english')\n",
    "sw_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = [pd.Series(x).value_counts() for x in words]\n",
    "word_df = pd.concat(words_list,axis=1).fillna(0).T\n",
    "word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords:\n",
    "words_nsw = word_df.loc[:,~word_df.T.index.isin(sw_list)]\n",
    "words_nsw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-swift",
   "metadata": {},
   "source": [
    "### Words counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_nsw.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_nsw.sum().sort_values().tail(20).plot(kind='barh',figsize=(7,5));\n",
    "plt.xlabel(\"Number of times\");\n",
    "plt.title(\"Frequently used words in Zillow listings \\n Houses for rent. Champaign\",fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-tissue",
   "metadata": {},
   "source": [
    "### An application of Term Frequency - Inverse Document Frequency\n",
    "\n",
    "In this case, a document will be a row of our original data. (i.e., one listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-apparel",
   "metadata": {},
   "source": [
    "**Term Frequency - Inverse Document Frequency (TF-IDF)** \n",
    "\n",
    "Term frequency: how often does a word appear in a document?\n",
    "\n",
    "Document frequency: How many documents contain this word?\n",
    "\n",
    "We divide term frequency by the total number of documents that have that word: $TFDF = TF/DF$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_nswT = words_nsw.T\n",
    "words_nswT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_calc(column):\n",
    "    return column/column.sum()\n",
    "\n",
    "tf = words_nswT.apply(tf_calc,axis=1)\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now calculate IDF:\n",
    "inv_doc_freq = np.log(tf.shape[1]/(words_nswT!=0).sum(axis=1))\n",
    "inv_doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, using vectorization method:\n",
    "\n",
    "idf_mat= np.repeat(np.array(inv_doc_freq)[:,np.newaxis],\\\n",
    "                   tf.shape[1],\\\n",
    "                   axis=1)\n",
    "\n",
    "tf_idf = tf*idf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking one listing:\n",
    "listing_no= 10\n",
    "tf_idf[listing_no][tf_idf[listing_no]<5.9].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking words with highest values of TD-DF (overall)\n",
    "tf_idf['mean'] = tf_idf.mean(axis=1)\n",
    "tf_idf['mean'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf['mean'].sort_values(ascending=False).head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_high = tf_idf['mean'].sort_values(ascending=False).head(10)\n",
    "words_high = list(words_high.index)\n",
    "words_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idfT = tf_idf.T\n",
    "tf_idfT_sub = tf_idfT[words_high]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[words_high] = tf_idfT_sub\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-apparel",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = [sid.polarity_scores(x)['compound'] for x in df['description_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-vietnamese",
   "metadata": {},
   "source": [
    "### Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from econtools.metrics import reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['_cons'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg(df,\"price\",[\"_cons\",\"beds\", \"baths\",\"sq_feet\",\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-museum",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
