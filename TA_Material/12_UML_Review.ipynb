{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surprised-animal",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning\n",
    "## Asset Index Review\n",
    "\n",
    "The Living Standard Management Survey (LSMS) is a World Bank used for collecting household-level data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-dress",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/Analysis/assets.csv\")\n",
    "df=df.set_index(['household_id',\"ea_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-rouge",
   "metadata": {},
   "source": [
    "## 1. Principal component analysis (PCA)\n",
    "\n",
    "Check PCA documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "- Principal component analysis (PCA) is a mathematical procedure that transforms a number of (possibly) correlated variables into a (smaller) number of uncorrelated variables called principal components.\n",
    "- The first principal component accounts for as much of the variability in the data as possible, and each succeeding component accounts for as much of the remaining variability as possible \n",
    "- PCA is a dimensionality reduction or data compression method. The goal is dimension reduction and there is no guarantee that the dimensions are interpretable\n",
    "\n",
    "### 1.1 Standardize Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets=df\n",
    "assets_std = (assets - assets.mean())/assets.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_std.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-label",
   "metadata": {},
   "source": [
    "### 1.2 Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=20 # number of components (let's start with large number)\n",
    "pca_model = PCA(n_components=K)\n",
    "pca_model = pca_model.fit(assets_std.fillna(0))  # Fit the model with the data assets_std \n",
    "pca_model.transform(assets_std.fillna(0))  # This line applies the dimensionality reduction to the data.\n",
    "# i.e., it created a K-d coordinate system out of the several asset variables we have.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-seventh",
   "metadata": {},
   "source": [
    "### 1.3 Check explained variance and eigen values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-purchase",
   "metadata": {},
   "source": [
    "#### Variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1,K+1)),pca_model.explained_variance_ratio_.cumsum());\n",
    "plt.xlabel('Numbers of components');\n",
    "plt.ylabel('Explained variance (Cumulative)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-outdoors",
   "metadata": {},
   "source": [
    "#### Eigen Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1,K+1)),pca_model.explained_variance_);\n",
    "plt.axhline(1,ls=\"--\",color='black');\n",
    "plt.xlabel('Numbers of components');\n",
    "plt.ylabel('Eigen Values');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-bradley",
   "metadata": {},
   "source": [
    "### 1.4 Define number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6\n",
    "pca_model = PCA(n_components=K)\n",
    "pca_model = pca_model.fit(assets_std.fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-value",
   "metadata": {},
   "source": [
    "Factor loadings (factor or component coefficients) : The factor loadings, also called component loadings in PCA, are the correlation coefficients between the variables (rows) and factors (columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-month",
   "metadata": {},
   "source": [
    "### 1.5 Compute components coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_scores = pd.DataFrame(pca_model.components_)\n",
    "load_scores.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(load_scores.T[0]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-baseball",
   "metadata": {},
   "source": [
    "### 1.6 Apply PCA to the data (dimensionality reduction)\n",
    "\n",
    "Just for visualization purposes of this notebook. We will compute only TWO components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-sender",
   "metadata": {},
   "source": [
    "Here is Prof. Jared Code to interact with PCA model easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_Model:\n",
    "    def __init__(self,ncomps,data,cols):\n",
    "        \n",
    "        self.ncomps = ncomps\n",
    "        self.cols = cols\n",
    "        self.data = data\n",
    "        self.scaled_data = StandardScaler().fit_transform(data[cols].values)\n",
    "        \n",
    "    def fit(self):\n",
    "        self.PCA_obj = PCA(self.ncomps).fit(self.scaled_data)\n",
    "    \n",
    "    def describe(self):\n",
    "        explvar_ = self.PCA_obj.explained_variance_ratio_\n",
    "\n",
    "        explvar = pd.DataFrame(explvar_[np.newaxis,:]*100,\\\n",
    "                               columns=[\"Component \" + str(x) for x in range(self.ncomps)],\\\n",
    "                               index=[\"% Explained Variance\"])\n",
    "        \n",
    "        loadscores_ = self.PCA_obj.components_\n",
    "        loadscores = pd.DataFrame(loadscores_,\\\n",
    "                                  columns=self.cols,\\\n",
    "                                  index=[\"Component \" + str(x) for x in range(self.ncomps)])\n",
    "        print(\"Explains {0:0.2f}% of the variance\".format(round(sum(explvar_),2)*100))\n",
    "        print(explvar)\n",
    "        print(loadscores)\n",
    "        return explvar,loadscores\n",
    "    \n",
    "    def gen_data(self):\n",
    "        return self.PCA_obj.transform(self.scaled_data)\n",
    "    \n",
    "    def graph(self,alpha=.6):\n",
    "        self.graphdata = pd.concat([self.data[[color]].reset_index(drop=True),\\\n",
    "                                    pd.DataFrame(self.gen_data()).reset_index(drop=True)],axis=1)\n",
    "        if self.ncomps==2:\n",
    "            return self.graphdata.plot(kind=\"scatter\",x=0,y=1,\\\n",
    "                                       c=color,alpha=alpha,colormap=cm)\n",
    "        else:\n",
    "            ax = plt.axes(projection='3d')\n",
    "            ax.scatter3D(self.graphdata[0], \\\n",
    "                         self.graphdata[1], \\\n",
    "                         self.graphdata[2])#, \\\n",
    "                         #c=color,colormap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA_Model(2,assets.fillna(0),assets.columns)\n",
    "pca2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "components=pd.DataFrame(pca2.gen_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(components[0], components[1], s=10, alpha=0.5);\n",
    "plt.xlabel('First Component');\n",
    "plt.ylabel('Second Component');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-giving",
   "metadata": {},
   "source": [
    "## 2. K-Means\n",
    "\n",
    " - An algorithm to determine implicit grouping in data.\n",
    " - It minimizes the within-cluster squared Euclidean distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-seven",
   "metadata": {},
   "source": [
    "### First - We will review the class example \n",
    "\n",
    "### 2.1 Read/Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code creates \"clustered\" data already! (So we know the answer in advance)\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Where:\n",
    "# X = The generated samples.\n",
    "# y_true = The integer labels for cluster membership of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], s=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-postcard",
   "metadata": {},
   "source": [
    "### 2.2 Assign K and run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2)  # If we define 2 clusters (of course we know that is NOT the right number)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-soldier",
   "metadata": {},
   "source": [
    "Here is Prof. Jared code that makes the interaction with K-Means easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kmeans_Model:\n",
    "    def __init__(self,data,num_clusters):\n",
    "        self.df = StandardScaler().fit_transform(data)\n",
    "        self.K = num_clusters\n",
    "        \n",
    "    def fit(self):    \n",
    "        self.model = KMeans(self.K).fit(self.df)\n",
    "    \n",
    "    def predict(self):\n",
    "        self.prediction = self.model.predict(self.df)\n",
    "        return self.prediction\n",
    "    \n",
    "    def sil_score(self):\n",
    "        return silhouette_score(self.df,self.model.predict(self.df),sample_size=10000)\n",
    "    \n",
    "def score_by_k(data,K):\n",
    "    model = Kmeans_Model(data,K)\n",
    "    model.fit()\n",
    "    return model.sil_score()\n",
    "\n",
    "def inertia_by_k(data,K):\n",
    "    model = Kmeans_Model(data,K)\n",
    "    model.fit()\n",
    "    return model.model.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-badge",
   "metadata": {},
   "source": [
    "### How do we know the \"optimal\" number of clusters? \n",
    "\n",
    "\n",
    "### Heuristics for selection: Silhouette Scores\n",
    "\n",
    "Silhouette scores near +1 indicate that the sample is far away from the neighboring clusters (i.e., they are closest to their assigned cluster). A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=20\n",
    "sil_scores =[score_by_k(X,i) for i in range(2,K)];\n",
    "plt.plot(list(range(2,K)),sil_scores);\n",
    "plt.axvline(4,color=\"black\",ls=\"--\");\n",
    "plt.xlabel('Numbers of clusters K');\n",
    "plt.ylabel('Average Silhouette Width');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-transsexual",
   "metadata": {},
   "source": [
    "### Inertia: Is the function being minimized (Within-sum of squared distances) \n",
    "\n",
    " - Inertia can be recognized as a measure of how internally coherent clusters are. Lower values are better and zero is optimal.\n",
    " - Criteria: Choose the K at the \"elbow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_scores =[inertia_by_k(X,i) for i in range(2,K)];\n",
    "plt.plot(list(range(2,K)),init_scores);\n",
    "plt.axvline(4,color=\"black\",ls=\"--\");\n",
    "plt.xlabel('Numbers of clusters K');\n",
    "plt.ylabel('Sum of squared distances');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-consumer",
   "metadata": {},
   "source": [
    "## Assets example \n",
    "### Run K-means over the two components that we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "components.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-barbados",
   "metadata": {},
   "source": [
    "### Let's look at the heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=20\n",
    "sil_scores =[score_by_k(components,i) for i in range(2,K)];\n",
    "plt.plot(list(range(2,K)),sil_scores);\n",
    "plt.xlabel('Numbers of clusters K');\n",
    "plt.ylabel('Average Silhouette Width');\n",
    "plt.axvline(3,color=\"black\",ls=\"--\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_scores =[inertia_by_k(components,i) for i in range(2,K)];\n",
    "plt.plot(list(range(2,K)),init_scores);\n",
    "plt.axvline(3,color=\"black\",ls=\"--\");\n",
    "plt.xlabel('Numbers of clusters K');\n",
    "plt.ylabel('Sum of squared distances');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-photograph",
   "metadata": {},
   "source": [
    "### Visualization of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(components)\n",
    "y_kmeans = kmeans.predict(components)\n",
    "\n",
    "plt.scatter(components[0],components[1], c=y_kmeans, s=10, cmap='Pastel2')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=20);\n",
    "plt.xlabel('First Component');\n",
    "plt.ylabel('Second Component');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
