{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cropland Data Layer (CDL) and Random Forest\n",
    "\n",
    "A raster dataset available from the USDA that identifies crops and land uses from satelite imagery.\n",
    "\n",
    "- 30m resolution\n",
    "- Entire U.S.\n",
    "- All states from 2008 till today.\n",
    "\n",
    "A \"census by satelite.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do they identify land uses?\n",
    "\n",
    "An application of __supervised machine learning!__\n",
    "\n",
    "After extensive data is collected from several satelites (AWiFS and MODIS) and geological survey data (Landsat TM) the task is to use these measurments to do a classification. The goal is to classify a single pixel into one of the categories of land use.\n",
    "\n",
    "The following slides are from [this presentation](https://www.nass.usda.gov/Research_and_Science/Cropland/Method/cropland.pdf) from the USDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"cdl-inputs.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "__So they have the X, where is the y?__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"ag-ground.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "The Farm Service Agency provides the \"on-the-ground truth\" of which pixel is what kind of crop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"non-ag-ground.png\" width=\"500\">\n",
    "</center>\n",
    "The US Geological Service provides the \"on-the-ground truth\" for non-crop pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Works This Way:\n",
    "<center>\n",
    "<img src=\"prediction.png\" width=\"700\">\n",
    "</center>\n",
    "According to their presentation, they hold out 30% of the sample to do training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What algorithm do they use?\n",
    "\n",
    "From what I can tell, the CDL uses __decision trees__.\n",
    "\n",
    "How to use decision trees for prediction is a whole class by itself, but let's start with a brief explanation about the  concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees: A Bin Estimator Explanation.\n",
    "\n",
    "Suppose I have three discrete levels of one variable, altitude, while I will call $X$. The levels are $x_1, x_2, x_3$.\n",
    "\n",
    "Now suppose I have a variable that represents whether a pixel is a coffee plant ot no. Call it $y=\\{0,1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If I were looking at data like this:\n",
    "\n",
    "\n",
    "|  |     |     |          ||\n",
    "|:-|:-|:-|:-|:-|\n",
    "|X   |$x_1$  |$x_2$  |$x_3$  |$x_1$|  \n",
    "|y   | 0     | 0     | 1     | 0     |\n",
    "\n",
    "What might we conclude about using $X$ to predict $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we have more data:\n",
    "\n",
    "|  |     |     |          ||||\n",
    "|:-|:-|:-|:-|:-|:-|:-|\n",
    "|X   |$x_1$  |$x_2$  |$x_3$  |$x_1$|$x_2$| $x_3$|  \n",
    "|y   | 0     | 0     | 1     | 0     | 1 | 1|\n",
    "\n",
    "\n",
    "How has it changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like now the relationship is not so clear. Coffee is sometimes grown at $x_2$ and sometimes not...\n",
    "\n",
    "So to understand it better, I might try including another variable $Z$.\n",
    "\n",
    "|  |     |     |          ||||\n",
    "|:-|:-|:-|:-|:-|:-|:-|\n",
    "| Z  |$z_1$  |$z_1$  |$z_1$  |$z_2$|$z_2$| $z_2$|  \n",
    "|X   |$x_1$  |$x_2$  |$x_3$  |$x_1$|$x_2$| $x_3$|  \n",
    "|y   | 0     | 0     | 1     | 0     | 1 | 1|\n",
    "\n",
    "\n",
    "Now how has our view changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to make a good predictor of $y$, we could essentially just start splitting things into bins.\n",
    "\n",
    "For example:\n",
    "\n",
    "- $z_1$ or $z_2$?\n",
    "    - if $z_1$, \n",
    "        + is $x<x_3$? then $y=0$\n",
    "        + is $x>=x_3$? then $y=1$\n",
    "    - if $z_2$?\n",
    "        + is $x<x_2$? then $y=0$\n",
    "        + is $x>=x_2$? then $y=1$\n",
    "        \n",
    "This would be a perfectly acceptable estimator of $y$, and is in fact a __decision tree estimator__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example algorithm:\n",
    "- Make an initial split, which produces \"leaves.\"\n",
    "- Is the value of the dependent variable all one value?\n",
    "    - if yes, end that leaf.\n",
    "    - if no, do another split.\n",
    "- Continue until a certain condition is met or the algorithm perfectly predicts.\n",
    "\n",
    "__What's may be the problem with an algorithm that perfectly predicts?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Such an algorithm is probably overfitting the data__.\n",
    "\n",
    "For example, it may be coffee plants in a certain area are grown with specific conditions. The algorithm might mistakenly then identify __anything__ grown in those conditions as a coffee plant. Thus the algorithm \"overfits\" the data by making its identification too specific to the data it has.\n",
    "\n",
    "This is why we do K-fold cross validation, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"decision_tree_diagram.png\" width=\"600\">\n",
    "</center>\n",
    "    \n",
    "\n",
    "[From this article](https://towardsdatascience.com/decision-tree-intuition-from-concept-to-application-530744294bb6), which is cited in [this presentation](https://docs.google.com/presentation/d/1AB2Adsf0akogIisth58prYRrLXzBYshBkLYvZUqnlhk/edit#slide=id.g75470aa6df_0_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree terminology:\n",
    "- Leaf: the final category after splitting.\n",
    "- Splitting: subsetting the target variable based on the features.\n",
    "- Max depth: the maximum depth of the tree.\n",
    "- Node: the decision node at which the plit is being made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does it know when to split?\n",
    "\n",
    "Essentially, its looking for an \"information gain\" from splitting. One metric it can use is __Entropy:__ $ -\\sum^N_i p_i log_2(p_i)$ where there are $N$ labels to predict and $p_i$ is the probability of label $i$. \n",
    "<center>\n",
    "<img src=\"entropy.png\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "[figure source](https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is highest the more mixed a set of targets is. If the probability were always 1, then the entropy score would be zero (as it would be if the probability were always zero).\n",
    "\n",
    "Thus is our leaves are more \"ordered\" after a split, we made an __entropy gain__. An entropy gain is essentially: $Ent(y) - Ent(y|x)$ where $E$ is a measure of entropy.\n",
    "\n",
    "__The analog for a continuous $y$ would be mean squared error (MSE)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Recall that to find information gains the algorithm is just splitting features until it finds something. What if it splits the wrong way? How can we correct this?\n",
    "\n",
    "Since one decision tree could make errors, often times the predictions of several trees are averaged together to make a __random forest__. Since we are randomly splitting, the trees are arguably __uncorrelated__. It ends up working a lot like a bootstrap estimator.\n",
    "\n",
    "Also will stop it from __overfitting__ too much.\n",
    "\n",
    "Other methods for combining trees are __boosting__ and __ensembling__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example from [this link](https://blog.hyperiondev.com/index.php/2019/02/18/machine-learning/) where each image contains a number, specifically numbers from street signs. The data set is the [Street View House Numbers dataset](http://ufldl.stanford.edu/housenumbers/) and is managed by Stanford."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "# load our dataset\n",
    "# train_data = scipy.io.loadmat('../../Data/Images/train_32x32.mat')\n",
    "# extract the images and labels from the dictionary object\n",
    "# X = train_data['X']\n",
    "# y = train_data['y']\n",
    "# view an image (e.g. 25) and print its corresponding label\n",
    "img_index = 2019\n",
    "plt.imshow(X[:,:,:,img_index])\n",
    "plt.show()\n",
    "print(y[img_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these are small images (32x32) with RGB chanels. There are 73k of them in this dataset.\n",
    "\n",
    "This size will not work for the Random Forest algorithm, as we need a 2-dimensional array instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_re = X.reshape(X.shape[0]*X.shape[1]*X.shape[2],X.shape[3]).T\n",
    "y_re = y.reshape(y.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_re.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What are the features now?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically every possible pixel of the image (in each band) is its own feature. It does not seem intuitive that this would work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=500) # Giving it 500 trees, \n",
    "                                                   # which is one of the hyperparameters we can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now do the split that the CDL uses: 30% test data and 80% training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_re, y_re, test_size=0.2, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "preds = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the algorithm seems naive, as you can see it can be quite powerful. Some advantages of Random Forest:\n",
    "1. It is __intuitive__, unlike other supervised ML algorithms.\n",
    "    - The mechanics of how it works can be easily understood.\n",
    "2. It is __versatile__, in that it can do classification and regression.\n",
    "    - $y$ can be continuous or discrete.\n",
    "3. It is __fast__, in that it can train on large datasets with lots of features.\n",
    "    - Also easy to parallelize.\n",
    "4. No preprocessing needed (like normalization for example).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some drawbacks:\n",
    "1. No model interpretation (like most ML algorithms).\n",
    "2. Tends to overfit, meaning make sure you tune hyperparameters.\n",
    "3. Seems to be dumped in favor of neural nets more and more, possibly because:\n",
    "    - Neural nets can handle more types of data.\n",
    "    - Neural nets often outperform most algorithms.\n",
    "    \n",
    "However, it seems like Random Forests in general need less data than neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
