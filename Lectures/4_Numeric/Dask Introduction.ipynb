{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask\n",
    "\n",
    "### What is dask?\n",
    "A package in python that does __distributed programming__. It is useful as a higher level interface for doing complicated distributed programming in an intuitive way. Distributed programming is useful when you don't have the ability to read things entirely into memory, and so of greate use when we have large datasets.\n",
    "\n",
    "It is similar in function to __Apache Spark__ or the `pyspark` package for Python.\n",
    "\n",
    "Before we learned how to manually parallelize operations, but in `dask` it will more or less be done automatically. A great advantage is that the interface mimics `pandas` and `numpy` and so can be used easily.\n",
    "\n",
    "### Delayed tasks\n",
    "\n",
    "One of the first handy things that `dask` can do is do functions in a batch using the `delayed` framework\n",
    "\n",
    "Before doing that, we need to initiate a client for us to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "client = Client(threads_per_worker=2, n_workers=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that we wanted to run this sequence of functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def inc(x):\n",
    "    time.sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def dec(x):\n",
    "    time.sleep(1)\n",
    "    return x - 1\n",
    "\n",
    "def add(x, y):\n",
    "    time.sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = inc(1)\n",
    "y = dec(2)\n",
    "z = add(x, y)\n",
    "z "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did it take so long? It turns out that calling these functions takes quite a bit of time. Notice, however, that the function `inc` and the function `dec` could be called independently! You only need to add them at the end.\n",
    "\n",
    "The power of the dask delayed framework is its ability to lay out the best way to execute it before it needs to be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "inc_d = dask.delayed(inc)\n",
    "dec_d = dask.delayed(dec)\n",
    "add_d = dask.delayed(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inc_d(1)\n",
    "y = dec_d(2)\n",
    "z = add_d(x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't take any time at all. Why? Because it hasn't run it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize(rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_s = []\n",
    "for i in range(20):\n",
    "    x = inc(i)\n",
    "    y = dec(x)\n",
    "    z = add(x, y)\n",
    "    zs_s.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_f = []\n",
    "for i in range(20):\n",
    "    x = inc_d(i)\n",
    "    y = dec_d(x)\n",
    "    z = add_d(x, y)\n",
    "    zs_f.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = dask.compute(*zs_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Dask makes parallelizing quite easy!\n",
    "\n",
    "## Distributed Programming\n",
    "\n",
    "### Dask Dataframe\n",
    "\n",
    "One of the most useful implementations of `dask` is using the `dataframe` API, which mimicks an actual DataFrame.\n",
    "\n",
    "For this example and your homework, we will look at a bilateral trade flows dataset called BACI. We will be analyzing the years 2012-2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df18 = pd.read_csv(\"P:trade_data/BACI_HS12_Y2018_V202001.csv\")\n",
    "df18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents the trade flows from country $i$ to country $j$ for product $k$ in time $t$ in terms of either value or quantity.\n",
    "\n",
    "Since we have 8 million rows of data in one year, this means we can expect about 56 million rows in the final dataset. This will be hard to work with all in memory at the same time!\n",
    "\n",
    "Some options:\n",
    "1. Chunk all our operations and run on each file.\n",
    "2. Use the `dask` `dataframe` API framework.\n",
    "\n",
    "We got a sense for how to use option 1, so now let's go to option 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"P:trade_data/BACI_HS12_Y*.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice I used read_csv to find __all__ the files in that directory and read them into one structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "glob.glob(\"P:trade_data/BACI_HS12_Y*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why did it give me 42 partitions if I only have 7 csv files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.map_partitions(len).compute().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all the partitions are about the same number of obs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_usage(x):\n",
    "    return x.memory_usage().sum()\n",
    "\n",
    "df.map_partitions(mem_usage).compute().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And about the same memory usage (around 70-80 mb per partition).\n",
    "\n",
    "__Rule of thumb__: partitions should be around 100 mb each.\n",
    "\n",
    "Here `dask` tries to do it automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My strategic use of the `map_partitions` will map a function to each partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 7 of these files now exist as \"partitions\" in this dataframe object. Now when we do an operation it will be \"distributed\" to all the partitions.\n",
    "\n",
    "So now the advantage of this framework is that we can run operations on all the data but `dask` does the chunking and parallelizing for us!\n",
    "\n",
    "__Advantage__: no need to do everything in memory!\n",
    "\n",
    "__Disadvantage__: will be slower than in-memory.\n",
    "\n",
    "Thus only an advantage when your data will not fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How it works:\n",
    "- Run as many operations as you need to on the symbolic dataframe object.\n",
    "- Call \"compute\" when you actually need to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the read_csv function will have to run 42 times in the current flow.\n",
    "\n",
    "What happens if we add an operation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.t==2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it added a couple more tasks after I did a slicing.\n",
    "\n",
    "Let's add one more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.t==2018].groupby(\"k\")['q'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So taking a group average for the year 2018 takes 267 tasks.\n",
    "\n",
    "When we actually need the results, we can run `compute`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.t==2018].groupby(\"k\")['q'].mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can calculate a useful summary statistic of the entire dataset without having to read the entire dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persisting memory\n",
    "\n",
    "In case you want one set of results in memory, you can use the `persist` function to put your results in memory.\n",
    "\n",
    "Suppose I want to persist the product with the code 10121:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10121 = df[df.k==10121].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it will apply all these changes in the background so we can keep working. \n",
    " \n",
    "Notice that it's still not a real \"dataframe\" yet. This is because its keeping the results distributed across the nodes. We can keep calling more functions on this flow and when it's done it will finish much quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downside of this is that it has transferred some of this (but not all of it) into the distributed RAM.\n",
    "\n",
    "The upside here is that it can be called very quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10121.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the difference between `persist` and `compute`?\n",
    "\n",
    "The command `persist` puts it into distributed memory (each worker holding a part of the data) so there are still as many partitions as before. The command `compute` puts it into one python object in your environment. \n",
    "\n",
    "__Why we use `persist`__:\n",
    "- We want to call something into the distributed memory to calculate with `compute`.\n",
    "- We need it to run asynchronously.\n",
    "- We need to \"set up\" a computation that we will eventually compute.\n",
    "    - Essentially saves us from having to wait a very long time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing merges can be accomplished the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv(\"P:trade_data/product_codes_HS12_V202001.csv\")\n",
    "products = products.rename(columns={\"code\":\"k\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wdes = df.merge(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we've merged in a text description of our product codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.read_csv(\"P:trade_data/country_codes_V202001.csv\",encoding = \"ISO-8859-1\")\n",
    "countries = countries.rename(columns={\"country_code\":\"i\",\"country_name_full\":\"name_i\"})\n",
    "df_wdes = df_wdes.merge(countries[['i','name_i']])\n",
    "\n",
    "countries = pd.read_csv(\"P:trade_data/country_codes_V202001.csv\",encoding = \"ISO-8859-1\")\n",
    "countries = countries.rename(columns={\"country_code\":\"j\",\"country_name_full\":\"name_j\"})\n",
    "df_wdes = df_wdes.merge(countries[['j','name_j']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wdes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which dataframe operations the dask API supports:\n",
    "\n",
    "__Element by element operations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_price'] = df['v']/df['q']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since its delaying the operations, it won't run until I call `compute`, though notice that it just added a column to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Selecting and slicing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg18 = df.loc[df.t==2018]['avg_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call compute to actually make a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = avg18.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Statistical functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg18.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg18.var().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Typical pandas functionality__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['k'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['q'].nlargest(10).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `nlargest` returns the n largest elements of a column.\n",
    "\n",
    "It also does groupby in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"i\")['v'].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice another advantage of this framework is that you can do standard `pandas` operations but in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Bags\n",
    "This works with dataframes well, but about with python objects?\n",
    "\n",
    "This is what `bags` are for. Bags essentially hold a list of python objects which can be parsed with any function necessary. \n",
    "\n",
    "This is handier for something like text analysis or image analysis where our objects are unstructured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "import json\n",
    "\n",
    "doge = db.read_text('../../Data/Text/doge_tweets*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doge.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we've extracted a tweet! We could apply a function so that they become json objects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take each one individually, have to do some parsing of the strings so I wrote this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json(x):\n",
    "    if x[0] ==\"[\":\n",
    "        extract = x[1:-2]\n",
    "    elif x[-1] ==\"]\":\n",
    "        extract = x[:-1]\n",
    "    else:\n",
    "        extract = x[:-2]\n",
    "    return json.loads(extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doge = doge.map(make_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now what if we wanted to extract all of the timestamps from the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = doge.map(lambda x: x['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamps = times.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(time_stamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also write a function that processes the data in some way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_hashtags = re.compile(\"#\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(x):\n",
    "    text = x['text']\n",
    "    return find_hashtags.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doge_hash = doge.map(get_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = doge_hash.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    # Iterate through the outer list\n",
    "    for element in _2d_list:\n",
    "        if type(element) is list:\n",
    "            # If the element is of type list, iterate through the sublist\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_list = flatten_list(doge_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(hash_list).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter our tweets accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doge.filter(lambda x: pd.to_datetime(x['created_at']) > pd.to_datetime('2021-02-04T15:00:00-05:00')).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaway message: we can process any kind of data with dask without having to put anything in memory.\n",
    "\n",
    "In this case, we could be reading text using dask, processing it in parallel, and writing it out each time.\n",
    "\n",
    "When could we apply this?\n",
    "\n",
    "1. In Homework 1, we needed to read large amounts of tweets in. Rather than read them all in at once, we could have filtered them as json files using a `dask` \"bag.\"\n",
    "    - The Dask `bag` is frequently used in unstructured data processing.\n",
    "2. In Homework 2, we could have read several .tif files in and process them in a distributed framework. \n",
    "    - Very useful if you had several raster files that you need to read in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Arrays\n",
    "\n",
    "Just as there is a dataframe object, there is also a `dask` equivalent to a `numpy` array.\n",
    "\n",
    "As an example, let's make a giant array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "x = da.random.random((80000, 80000), chunks=(1000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My machine will not read this in since it exceeds the memory usage of my machine.\n",
    "\n",
    "However, that doesn't mean we can't calculate some basic statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.mean(axis=0)[::10].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These arrays support nearly every operation that numpy arrays support. Notably, it supports __linear algebra__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.array import linalg\n",
    "x = da.random.random((20000, 20000), chunks=(1000, 1000))\n",
    "x_inv = linalg.inv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inv[:100,:10].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to take an inverse of an array in parallel with minimal memory is __very powerful__ and __very useful__ in many cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the uses of dask arrays would be processing high granularity satelite data.\n",
    "\n",
    "Your workflow might look like:\n",
    "- Read in a numpy array from a raster.\n",
    "- Process the numpy array in the desired way (e.g. clipping, transformations).\n",
    "- Write out the processed file.\n",
    "\n",
    "Potentially useful for doing linear algebra on very large arrays as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Application of Dask\n",
    "- Machine learning\n",
    "    - Dask has a package that blends seamlessly with the `sklearn` machine learning packages, which makes processing, cross-validation, and hyperparameter tuning all possible in the `dask` framework.\n",
    "- Cluster programming\n",
    "    - One of the best uses of `dask` is working on __multiple computers__, which is what this framework is usually used for.\n",
    "    - While its useful on a desktop, it is the most powerful when you have multiple clusters on a server you can use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Topic: Using Clusters and Cloud Computing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
